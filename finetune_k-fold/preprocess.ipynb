{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove url pattern\n",
    "pattern = r\"[(https?:\\/\\/)|(www\\.)]+[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\"\n",
    "\n",
    "\n",
    "def get_dataset(data_path):\n",
    "    \"\"\" Load data and return dataset for training and validating.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data.\n",
    "    Return:\n",
    "        output (list of dict): [dict, dict, dict ...]\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(data_path, dtype=str)\n",
    "    \n",
    "    formatData = []\n",
    "    for (idx, data) in dataset.iterrows():\n",
    "        \"\"\"\n",
    "        processed: {\n",
    "            'Abstract': [[4,5,6],[3,4,2],...],\n",
    "            'Label': [[0,0,0,1,1,0],[1,0,0,0,1,0],...]\n",
    "        }\n",
    "        \n",
    "        -> if test data\n",
    "        processed: {\n",
    "            'Abstract': [[4,5,6],[3,4,2],...],\n",
    "            'Id': T0001\n",
    "        }\n",
    "        \"\"\"\n",
    "        processed = {}\n",
    "        processed[\"Abstract\"] = [re.sub(pattern, \" \", sent) for sent in data[\"Abstract\"].split(\"$$$\")]\n",
    "        if \"Task 1\" in data:\n",
    "            processed[\"Label\"] = [label_to_onehot(label) for label in data[\"Task 1\"].split(\" \")]\n",
    "        else:\n",
    "            processed[\"Id\"] = data[\"Id\"]\n",
    "        formatData.append(processed)\n",
    "    return formatData\n",
    "\n",
    "\n",
    "def label_to_onehot(labels):\n",
    "    \"\"\" Convert label to onehot .\n",
    "        Args:\n",
    "            labels (string): sentence's labels.\n",
    "        Return:\n",
    "            outputs (onehot list): sentence's onehot label.\n",
    "    \"\"\"\n",
    "    label_dict = {\"BACKGROUND\": 0, \"OBJECTIVES\": 1, \"METHODS\": 2, \"RESULTS\": 3, \"CONCLUSIONS\": 4, \"OTHERS\": 5}\n",
    "    onehot = [0] * 6\n",
    "    for lable in labels.split(\"/\"):\n",
    "        onehot[label_dict[lable]] = 1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    RandomSampler,\n",
    "    SequentialSampler,\n",
    "    TensorDataset\n",
    ")\n",
    "from transformers import (\n",
    "    AlbertTokenizer,\n",
    "    BertTokenizer, \n",
    "    RobertaTokenizer, \n",
    "    XLNetTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(\n",
    "    examples,\n",
    "    max_length,\n",
    "    tokenizer,\n",
    "    pad_token_segment_id=0,\n",
    "    pad_on_left=False,\n",
    "    pad_token=0,\n",
    "    mask_padding_with_zero=True,\n",
    "    do_lower=False,\n",
    "    is_test_data=False,\n",
    "    extra_datas=None,\n",
    "    model_mode=\"bert\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a data file into a list of `InputFeatures`\n",
    "    \"\"\"\n",
    "    \n",
    "    def process(sentence, abstract):\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            abstract,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_length - len(input_ids)\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            attention_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + attention_mask\n",
    "            token_type_ids = ([pad_token_segment_id] * padding_length) + token_type_ids\n",
    "        else:\n",
    "            input_ids = input_ids + ([pad_token] * padding_length)\n",
    "            attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "        assert len(input_ids) == max_length\n",
    "        assert len(attention_mask) == max_length\n",
    "        assert len(token_type_ids) == max_length\n",
    "        \n",
    "        return input_ids, attention_mask, token_type_ids\n",
    "            \n",
    "\n",
    "    pad_on_left=True if model_mode == \"xlnet\" else False # pad on the left for xlnet\n",
    "    pad_token_segment_id=4 if model_mode == \"xlnet\" else 0\n",
    "    features = []\n",
    "    if extra_datas:\n",
    "        for extra_data in tqdm(extra_datas, desc=\"convert examples to features\"):\n",
    "                abstract = extra_data.get(\"Abstract\", [])\n",
    "                sentence = extra_data.get(\"Sent\", [])\n",
    "                label = extra_data.get(\"Label\", [])\n",
    "                \n",
    "                if do_lower:\n",
    "                    abstract = abstract.lower()\n",
    "                    sentence = sentence.lower()\n",
    "\n",
    "                input_ids, attention_mask, token_type_ids = process(sentence, abstract)\n",
    "                features.append(\n",
    "                    {\n",
    "                        \"net_inputs\": {\n",
    "                            \"input_ids\": input_ids,\n",
    "                            \"attention_mask\": attention_mask, \n",
    "                            \"token_type_ids\": token_type_ids\n",
    "                        },\n",
    "                        \"label\": label\n",
    "                    }\n",
    "                )       \n",
    "    for example in tqdm(examples, desc=\"convert examples to features\"):\n",
    "        abstract = example.get(\"Abstract\", [])\n",
    "        assert len(abstract) > 0, \"no abstract data!\"\n",
    "        if do_lower:\n",
    "            abstract = [sent.lower() for sent in abstract]\n",
    "\n",
    "        sentences = abstract\n",
    "        abstract = \"\".join(abstract)\n",
    "\n",
    "        if is_test_data:\n",
    "            abstract_id = example.get(\"Id\", None)\n",
    "            assert abstract_id is not None, \"No abstract_id!\"\n",
    "\n",
    "            for idx, sentence in enumerate(sentences):\n",
    "                input_ids, attention_mask, token_type_ids = process(sentence, abstract)\n",
    "                features.append(\n",
    "                    {\n",
    "                        \"abstract_id\": abstract_id,\n",
    "                        \"seq_id\": idx+1, # start from 1\n",
    "                        \"net_inputs\": {\n",
    "                            \"input_ids\": input_ids,\n",
    "                            \"attention_mask\": attention_mask, \n",
    "                            \"token_type_ids\": token_type_ids\n",
    "                        },\n",
    "                    }\n",
    "                )\n",
    "        else:\n",
    "            labels = example.get(\"Label\", None)\n",
    "            assert labels is not None, \"No label data!\"\n",
    "\n",
    "            for sentence, label in zip(sentences, labels):\n",
    "                input_ids, attention_mask, token_type_ids = process(sentence, abstract)\n",
    "                features.append(\n",
    "                    {\n",
    "                        \"net_inputs\": {\n",
    "                            \"input_ids\": input_ids,\n",
    "                            \"attention_mask\": attention_mask, \n",
    "                            \"token_type_ids\": token_type_ids\n",
    "                        },\n",
    "                        \"label\": label\n",
    "                    }\n",
    "                )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_feature(k_id, data_dir, pretrained_weights, save_dir=\"data_bin\", do_lower=False, max_length=512):\n",
    "    tokenizer_dict = {\n",
    "        \"albert\": AlbertTokenizer,\n",
    "        \"bert\": BertTokenizer,\n",
    "        \"roberta\": RobertaTokenizer,\n",
    "        \"xlnet\": XLNetTokenizer,\n",
    "    }\n",
    "    \n",
    "    mode_dict = {\n",
    "        \"albert\": \"albert\",\n",
    "        \"bert\": \"bert\",\n",
    "        \"roberta\": \"roberta\",\n",
    "        \"xlnet\": \"xlnet\",\n",
    "    }\n",
    "\n",
    "    # load tokeinzer\n",
    "    model_tag = pretrained_weights.split(\"-\")[0]\n",
    "    tokenizer = tokenizer_dict[model_tag].from_pretrained(pretrained_weights)\n",
    "    \n",
    "    # load dataset\n",
    "    train = get_dataset(os.path.join(data_dir, \"trainset_{}.csv\".format(k_id)))\n",
    "    valid = get_dataset(os.path.join(data_dir, \"validset_{}.csv\".format(k_id)))\n",
    "    \n",
    "    # process\n",
    "    train_features = convert_examples_to_features(examples=train, max_length=max_length, tokenizer=tokenizer, model_mode=mode_dict[model_tag], do_lower=do_lower)\n",
    "    valid_features = convert_examples_to_features(examples=valid, max_length=max_length, tokenizer=tokenizer, model_mode=mode_dict[model_tag], do_lower=do_lower)\n",
    "    \n",
    "    \n",
    "    save_feature_to_bin(train_features, save_dir, tag=pretrained_weights + \"_{}\".format(k_id), version=\"v2\")\n",
    "    save_feature_to_bin(valid_features, save_dir, split=\"valid\", tag=pretrained_weights + \"_{}\".format(k_id), version=\"v2\")\n",
    "\n",
    "\n",
    "def save_feature_to_bin(features, save_dir, split=\"train\", tag=\"\", version=\"v1\"):\n",
    "    torch.save(features, os.path.join(save_dir, \"{}_{}_{}.pt\".format(split, tag, version)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"datasets/k10\"\n",
    "pretrained_weights = \"roberta-large\" \n",
    "\n",
    "func = partial(convert_data_to_feature, data_dir=data_dir, pretrained_weights=pretrained_weights)\n",
    "with Pool(10) as pool:\n",
    "    pool.map(func, range(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
